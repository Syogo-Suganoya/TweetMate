import re
import urllib.request
from argparse import ArgumentParser

from dotenv import load_dotenv
from tinysegmenter import TinySegmenter

from lib import x_api


class CommonKeywordDataset:
    def __init__(self):
        self.user1 = None
        self.user2 = None
        self.tweets1 = []
        self.tweets2 = []

        self._load_metadata()

    def _load_metadata(self):
        if args.mock:
            self.user1 = "634829374928374657"
            self.user2 = "783295847239182374"
            get_posts_method = x_api.get_user_posts_mock
        else:
            self.user1 = args.user1
            self.user2 = args.user1
            get_posts_method = x_api.get_user_posts
        self.tweets1 = [extract_post_text(t.text) for t in get_posts_method(self.user1).data]
        self.tweets2 = [extract_post_text(t.text) for t in get_posts_method(self.user2).data]


# æ—¥æœ¬èªã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’å–å¾—ï¼ˆGitHubã®stopwords-jaã‚’ä½¿ç”¨ï¼‰
def load_stopwords():
    url = "https://raw.githubusercontent.com/stopwords-iso/stopwords-ja/master/stopwords-ja.txt"
    response = urllib.request.urlopen(url)
    return set(response.read().decode("utf-8").splitlines())


# å…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºé–¢æ•°
def extract_common_keywords(texts1, texts2, top_n=10):
    stopwords_ja = load_stopwords()
    segmenter = TinySegmenter()
    tokens1 = segmenter.tokenize(" ".join(texts1))
    tokens2 = segmenter.tokenize(" ".join(texts2))
    keywords1 = [t for t in tokens1 if t not in stopwords_ja and len(t.strip()) > 1]
    keywords2 = [t for t in tokens2 if t not in stopwords_ja and len(t.strip()) > 1]
    return list(set(keywords1) & set(keywords2))[:top_n]


# ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†
def extract_post_text(text):
    text = re.sub(r"http\S+", "", text)  # URLå‰Šé™¤
    text = re.sub(r"[@#]\S+", "", text)  # @ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ã‚„#ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°å‰Šé™¤
    return text.lower()


def parse_args():
    parser = ArgumentParser(description="ã‚ªãƒ¢ã‚¤ã‚¢ã‚¤ å…±é€šç‚¹ç™ºè¦‹ãƒ„ãƒ¼ãƒ«")

    parser.add_argument("--mock", action="store_true", help="ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹")

    parser.add_argument("--user1", type=str, help="ä¸€äººç›®ã®Xã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å")
    parser.add_argument("--user2", type=str, help="äºŒäººç›®ã®Xã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å")

    args = parser.parse_args()

    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    if args.mock:
        # ãƒ¢ãƒƒã‚¯ä½¿ç”¨æ™‚ã¯ä»–ã®å¼•æ•°ã‚’æŒ‡å®šã—ã¦ã¯ã„ã‘ãªã„
        if any([args.user1, args.user2]):
            parser.error("--mock ã‚’æŒ‡å®šã—ãŸå ´åˆã€--user1ã€--user2 ã¯æŒ‡å®šã§ãã¾ã›ã‚“ã€‚")
    else:
        # å®Ÿãƒ‡ãƒ¼ã‚¿ä½¿ç”¨æ™‚ã¯å…¨ã¦ã®å¼•æ•°ãŒå¿…è¦
        missing = [opt for opt in ["user1", "user2"] if getattr(args, opt) is None]
        if missing:
            parser.error(f"--mock ã‚’ä½¿ã‚ãªã„å ´åˆã€ä»¥ä¸‹ã®å¼•æ•°ãŒå¿…è¦ã§ã™: {', '.join('--' + m for m in missing)}")

    return args


def main():
    print("å…±é€šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºä¸­...")
    dataset = CommonKeywordDataset()
    keywords = extract_common_keywords(dataset.tweets1, dataset.tweets2)
    print(f"\nğŸ§© {dataset.user1} ã•ã‚“ã¨ {dataset.user2} ã•ã‚“ã®å…±é€šãƒˆãƒ”ãƒƒã‚¯å€™è£œï¼š")
    print(keywords)


if __name__ == "__main__":
    load_dotenv()
    args = parse_args()
    main()
